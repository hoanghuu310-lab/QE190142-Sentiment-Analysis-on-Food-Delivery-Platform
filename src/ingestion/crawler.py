import requests
import json
import time
import random
import os
from schema_sentiment import ReviewItem

# --- C·∫§U H√åNH ---
DATA_FOLDER = "data_sentiment"
if not os.path.exists(DATA_FOLDER):
    os.makedirs(DATA_FOLDER)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'X-Requested-With': 'XMLHttpRequest',
    'x-foody-client-type': '1',
    'x-foody-client-version': '3.0.0',
    'x-foody-api-version': '1',
}

# B·∫¢NG T·ª™ ƒêI·ªÇN MAP T·ª™ URL -> ID TH√ÄNH PH·ªê
CITY_MAPPING = {
    "ha-noi": {"id": 218, "name": "HaNoi"},
    "ho-chi-minh": {"id": 217, "name": "HCM"},
    "da-nang": {"id": 219, "name": "DaNang"},
    "hai-phong": {"id": 220, "name": "HaiPhong"},
    # C√≥ th·ªÉ th√™m c√°c t·ªânh kh√°c n·∫øu c·∫ßn
}

def analyze_url(url):
    """
    Ph√¢n t√≠ch URL ƒë·ªÉ t√°ch Slug v√† Th√†nh ph·ªë
    Input: https://shopeefood.vn/ha-noi/pho-thin-lo-duc
    Output: slug='pho-thin-lo-duc', city_info={'id': 218, 'name': 'HaNoi'}
    """
    # X√≥a ph·∫ßn https://shopeefood.vn/
    clean_url = url.replace("https://shopeefood.vn/", "").replace("http://shopeefood.vn/", "")
    parts = clean_url.split("/")
    
    # URL chu·∫©n th∆∞·ªùng l√†: [ten-thanh-pho]/[ten-quan]
    if len(parts) >= 2:
        city_slug = parts[0]
        restaurant_slug = parts[1].split("?")[0] # B·ªè tham s·ªë ? sau slug
        
        # Tra c·ª©u trong t·ª´ ƒëi·ªÉn
        city_info = CITY_MAPPING.get(city_slug)
        if city_info:
            return restaurant_slug, city_info
            
    return None, None

def get_restaurant_id_from_slug(slug):
    """G·ªçi API ƒë·ªÉ ƒë·ªïi t√™n qu√°n (slug) th√†nh ID s·ªë"""
    url = f"https://gappapi.deliverynow.vn/api/delivery/get_detail?request_id={slug}&id_type=2"
    try:
        resp = requests.get(url, headers=HEADERS)
        data = resp.json()
        delivery_detail = data.get('reply', {}).get('delivery_detail', {})
        
        return {
            "id": delivery_detail.get('delivery_id'),
            "name": delivery_detail.get('name')
        }
    except:
        return None

def crawl_reviews_by_link(url_list, limit_per_shop=100):
    print(f"üöÄ ƒêang x·ª≠ l√Ω danh s√°ch {len(url_list)} qu√°n ƒÉn...")
    
    for url in url_list:
        print(f"\nüîó Checking: {url}")
        
        # 1. T·ª± ƒë·ªông ph√°t hi·ªán th√†nh ph·ªë
        slug, city_info = analyze_url(url)
        
        if not city_info:
            print("   ‚ö†Ô∏è Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c th√†nh ph·ªë t·ª´ Link n√†y. B·ªè qua.")
            continue
            
        print(f"   -> Ph√°t hi·ªán: {city_info['name']} (Slug: {slug})")
        
        # 2. L·∫•y ID qu√°n
        shop_info = get_restaurant_id_from_slug(slug)
        if not shop_info or not shop_info['id']:
            print("   ‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c ID qu√°n. Link c√≥ th·ªÉ b·ªã l·ªói.")
            continue
            
        shop_id = shop_info['id']
        shop_name = shop_info['name']
        
        # 3. T·∫°o t√™n file t·ª± ƒë·ªông theo th√†nh ph·ªë (T·ª∞ ƒê·ªòNG PH√ÇN LO·∫†I T·ªÜP KH√ÅCH H√ÄNG)
        output_file = os.path.join(DATA_FOLDER, f"reviews_{city_info['name']}.jsonl")
        
        # 4. Crawl Review
        print(f"   -> ƒêang t·∫£i review cho qu√°n: {shop_name}...")
        api_review = f"https://gappapi.deliverynow.vn/api/delivery/get_reply?id_type=1&request_id={shop_id}&sort_type=1&limit={limit_per_shop}"
        
        try:
            res = requests.get(api_review, headers=HEADERS)
            reviews = res.json().get('reply', {}).get('reply_list', [])
            
            if not reviews:
                print("   ‚ö†Ô∏è Qu√°n n√†y ch∆∞a c√≥ review n√†o.")
                continue

            with open(output_file, 'a', encoding='utf-8') as f:
                for rev in reviews:
                    item = ReviewItem(
                        review_id=rev.get('id'),
                        restaurant_id=shop_id,
                        restaurant_name=shop_name,
                        city=city_info['name'], # L∆∞u t√™n th√†nh ph·ªë v√†o t·ª´ng d√≤ng
                        user_name=rev.get('name', 'Anonymous'),
                        comment=rev.get('comment', ''),
                        rating=rev.get('rating', 0),
                        review_date=rev.get('created_on', '')
                    )
                    f.write(item.to_json_line() + "\n")
            
            print(f"   ‚úÖ ƒê√£ l∆∞u {len(reviews)} reviews v√†o file: reviews_{city_info['name']}.jsonl")
            
        except Exception as e:
            print(f"   ‚ùå L·ªói crawl review: {e}")
            
        # Ngh·ªâ nh·∫π ƒë·ªÉ kh√¥ng b·ªã spam
        time.sleep(random.uniform(1, 3))

# --- MAIN RUN ---
if __name__ == "__main__":
    
    # B·∫†N CH·ªà C·∫¶N D√ÅN LIST LINK V√ÄO ƒê√ÇY (L·ªòN X·ªòN C≈®NG ƒê∆Ø·ª¢C)
    # Code s·∫Ω t·ª± t√°ch: Link n√†o H√† N·ªôi -> V√†o file HaNoi, Link n√†o HCM -> V√†o file HCM
    
    MY_LINKS = [
        # Link H√† N·ªôi
        "https://shopeefood.vn/ha-noi/pho-thin-lo-duc", 
        "https://shopeefood.vn/ha-noi/bun-cha-dac-kim-hang-manh",
        
        # Link S√†i G√≤n
        "https://shopeefood.vn/ho-chi-minh/com-tam-cali-nguyen-trai-q1",
        "https://shopeefood.vn/ho-chi-minh/phuc-long-lotte-mart-le-dai-hanh",
        
        # Link ƒê√† N·∫µng
        "https://shopeefood.vn/da-nang/my-quang-ba-mua-tran-binh-trong"
    ]
    
    crawl_reviews_by_link(MY_LINKS, limit_per_shop=50)
