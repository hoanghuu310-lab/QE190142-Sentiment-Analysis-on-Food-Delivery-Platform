
B·∫°n ƒë√£ n√≥i:
ƒê√£ t·∫£i l√™n ·∫£nh
h√£y ƒë·ªçc v√† ph√¢n t√≠ch topic 2
ChatGPT ƒë√£ n√≥i:
M√¨nh ƒë√£ ƒë·ªçc CH·ª¶ ƒê·ªÄ 2: PH√ÇN T√çCH REVIEW & C·∫¢M X√öC (E-COMMERCE / FOOD). D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt + g·ª£i √Ω c√°ch tri·ªÉn khai (ƒë√∫ng ki·ªÉu b√†i Data/DS cho sinh vi√™n SE ·ªü FPT üëç).

1. M·ª•c ti√™u c·ªßa Topic 2 (Hi·ªÉu b√†i to√°n)
Topic n√†y xoay quanh khai th√°c v√† ph√¢n t√≠ch c·∫£m x√∫c kh√°ch h√†ng th√¥ng qua review tr√™n c√°c n·ªÅn t·∫£ng TMƒêT/ƒë·ªì ƒÉn.

üëâ Tr·∫£ l·ªùi c√°c c√¢u h·ªèi:

Ch·∫•t l∆∞·ª£ng d·ªãch v·ª•/s·∫£n ph·∫©m thay ƒë·ªïi theo th·ªùi gian kh√¥ng?

Kh√°ch ch√™ v√† khen ƒëi·ªÅu g√¨ nhi·ªÅu nh·∫•t?

C√≥ kh√°c bi·ªát h√†nh vi review theo v√πng mi·ªÅn hay kh√¥ng?

Review d√†i/ng·∫Øn c√≥ li√™n quan ƒë·∫øn m·ª©c ƒë·ªô h√†i l√≤ng kh√¥ng?

2. D·ªØ li·ªáu c·∫ßn crawl (Input Data)
üìå Ngu·ªìn d·ªØ li·ªáu
ShopeeFood

Foody

Shopee

Google Maps Reviews

üìå C√°c tr∆∞·ªùng b·∫Øt bu·ªôc
Tr∆∞·ªùng	√ù nghƒ©a
content	N·ªôi dung review
rating	S·ªë sao (1‚Äì5)
date	Ng√†y review
shop_name	T√™n qu√°n/shop
üëâ C√≥ th·ªÉ m·ªü r·ªông:

location (qu·∫≠n/t·ªânh)

reply_from_shop

platform

3. Ph√¢n t√≠ch C√ÇU H·ªéI C·ª®NG
üîπ 3.1. T·ª∑ l·ªá rating 1‚≠ê vs 5‚≠ê theo th·ªùi gian
M·ª•c ti√™u

Xem ch·∫•t l∆∞·ª£ng d·ªãch v·ª• c√≥ c·∫£i thi·ªán hay xu·ªëng c·∫•p kh√¥ng

C√°ch l√†m

Group theo th√°ng ho·∫∑c qu√Ω

T√≠nh:

% review 1 sao

% review 5 sao

V·∫Ω:

Line chart

Stacked bar chart

Insight mong ƒë·ª£i

1‚≠ê tƒÉng ƒë·ªôt bi·∫øn ‚Üí c√≥ th·ªÉ do:

Thay ƒë·ªïi shipper

TƒÉng gi√°

Ch·∫•t l∆∞·ª£ng gi·∫£m

5‚≠ê tƒÉng ‚Üí qu√°n c·∫£i thi·ªán d·ªãch v·ª•

üîπ 3.2. Top 10 keyword trong review 1‚≠ê v√† 5‚≠ê
M·ª•c ti√™u
üëâ Hi·ªÉu nguy√™n nh√¢n b·ªã ch√™ / ƒë∆∞·ª£c khen

C√°ch l√†m

T√°ch review:

Nh√≥m 1‚≠ê

Nh√≥m 5‚≠ê

Ti·ªÅn x·ª≠ l√Ω NLP:

Lowercase

Remove stopwords

Tokenization

Th·ªëng k√™ t·∫ßn su·∫•t t·ª´

V√≠ d·ª• k·∫øt qu·∫£

‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê:

‚Äúngon‚Äù, ‚Äúnhi·ªÅu‚Äù, ‚Äúr·∫ª‚Äù, ‚Äúship nhanh‚Äù, ‚Äúƒë√≥ng g√≥i k·ªπ‚Äù

‚≠ê:

‚Äúngu·ªôi‚Äù, ‚Äútr·ªÖ‚Äù, ‚Äú√≠t‚Äù, ‚Äúm·∫∑n‚Äù, ‚Äúshipper‚Äù

üëâ C√≥ th·ªÉ v·∫Ω:

WordCloud

Bar chart top keywords

4. Ph√¢n t√≠ch C√ÇU H·ªéI M·ªû (ƒÇn ƒëi·ªÉm)
üîπ 4.1. Kh√°c bi·ªát v√πng mi·ªÅn (B·∫Øc / Nam)
Gi·∫£ thuy·∫øt

Mi·ªÅn B·∫Øc:

Review d√†i

Hay g√≥p √Ω chi ti·∫øt

Mi·ªÅn Nam:

Review ng·∫Øn

ƒê√°nh gi√° c·∫£m t√≠nh

Ph√¢n t√≠ch

So s√°nh:

ƒê·ªô d√†i comment trung b√¨nh

T·ª∑ l·ªá 1‚≠ê / 5‚≠ê

Test th·ªëng k√™:

T-test / Mann-Whitney

üîπ 4.2. ƒê·ªô d√†i comment c√≥ li√™n quan ƒë·∫øn s·ªë sao kh√¥ng?
C√°ch l√†m

T√≠nh:

comment_length = s·ªë t·ª´

Scatter plot:

X: s·ªë sao

Y: ƒë·ªô d√†i

Correlation (Pearson / Spearman)

Insight th∆∞·ªùng g·∫∑p

Review 1‚≠ê ‚Üí d√†i (b·ª©c x√∫c)

Review 5‚≠ê ‚Üí ng·∫Øn (‚Äúngon‚Äù, ‚Äúok‚Äù)

5. K·ªπ thu·∫≠t & C√¥ng c·ª• n√™n d√πng
üìå Crawl
Selenium / Playwright

Google Maps API (n·∫øu c√≥ key)

üìå X·ª≠ l√Ω
Python: pandas, regex

NLP:

underthesea (ti·∫øng Vi·ªát)

VnCoreNLP

üìå Ph√¢n t√≠ch n√¢ng cao (n·∫øu mu·ªën n·ªïi b·∫≠t)
Sentiment classification (rule-based ho·∫∑c ML)

Topic Modeling (LDA)

So s√°nh gi·ªØa platforms

6. V√¨ sao Topic 2 r·∫•t ‚Äúngon‚Äù cho b√°o c√°o?
‚úÖ D·ªØ li·ªáu d·ªÖ ki·∫øm
‚úÖ C√≥ NLP + Time Series
‚úÖ Insight th·ª±c t·∫ø, d·ªÖ tr√¨nh b√†y
‚úÖ R·∫•t h·ª£p ƒë·ªÉ:

Demo EDA

Visualization

Business insight

N·∫øu b·∫°n mu·ªën, m√¨nh c√≥ th·ªÉ:

‚úÖ Vi·∫øt outline b√°o c√°o cho topic 2

‚úÖ ƒê·ªÅ xu·∫•t dataset m·∫´u

‚úÖ Vi·∫øt code pipeline (crawl ‚Üí clean ‚Üí analyze)

‚úÖ G·ª£i √Ω slide thuy·∫øt tr√¨nh

üëâ B·∫°n ƒëang l√†m b√°o c√°o m√¥n n√†o (AIL / DSA / Data Mining)?

B·∫°n ƒë√£ n√≥i:
for _ in range(5):  # Thu 5 cap parent
                if current is None:
                    break

                # Lay tat ca text trong container
                text = current.get_text(separator="\n", strip=True)

                # Neu co text dai hon, lay no
                if len(text) > len(content_text):
                    content_text = text

                current = current.find_parent()

            if not content_text:
                continue

            # Tach content thanh cac dong
            lines = content_text.split('\n')

            # Tim dong dau tien SAU username va khong phai metadata
            content_lines = []
            found_user = False

            skip_words = ['G√Ä', 'ƒê·∫†I B√ÄNG', 'B√°o x·∫•u', 'ng√†y', 'gi·ªù', 'Th√≠ch',
                          'Kh√¥ng th√≠ch', 'Share', 'FactTinhte', 'ƒë√£ n√≥i', 'said']

            for line in lines:
                line = line.strip()

                if not line:
                    continue

                # Neu gap username, bat dau ghi nhan
                if username in line:
                    found_user = True
                    continue

                # Sau khi gap username
                if found_user:
                    # Bo qua metadata
                    if any(word in line for word in skip_words):
                        continue

                    # Bo qua so don doc
                    if re.match(r'^\d+$', line):
                        continue

                    # Bo qua dong ngan (< 10 ky tu)
                    if len(line) < 10:
                        continue

                    # Day la noi dung
                    content_lines.append(line)

                    # Chi lay toi da 3 dong
                    if len(content_lines) >= 3:
                        break

            # Gop noi dung
            content = ' '.join(content_lines).strip()

            # Lam sach
            content = re.sub(r'@\w+', '', content)
            content = re.sub(r'[‚Üë‚Üí‚Üê‚Üì]', '', content)
            content = re.sub(r'\s+', ' ', content)

            # Chi them neu co noi dung hop le
            if content and len(content) > 15:
                processed.add(username)

                data.append({
                    "index": len(data) + 1,
                    "username": username,
                    "content": content
                })

                print("=" * 60)
                print(f"Post #{len(data)} | {username}")
                print(content[:200])

                if len(data) >= limit_posts:
                    break

        return data

    except Exception as e:
        print(f"Loi: {e}")
        import traceback
        traceback.print_exc()
        return []

    finally:
        driver.quit()


if __name__ == "__main__":
    print("Bat dau crawl Tinhte.vn...")
    posts = crawl_tinhte(limit_posts=20)

    if posts:
        with open("tinhte_posts.json", "w", encoding="utf-8") as f:
json.dump(posts, f, ensure_ascii=False, indent=2)
        print(f"\nDa luu {len(posts)} posts vao tinhte_posts.json")
    else:
        print("\nKhong crawl duoc du lieu!")
        print("Hay kiem tra file tinhte_selenium.html")
ChatGPT ƒë√£ n√≥i:
M√¨nh s·∫Ω ƒë·ªçc v√† ph√¢n t√≠ch chi ti·∫øt ƒëo·∫°n code b·∫°n g·ª≠i, t·∫≠p trung v√†o m·ª•c ƒë√≠ch, lu·ªìng x·ª≠ l√Ω, ƒëi·ªÉm m·∫°nh ‚Äì ƒëi·ªÉm y·∫øu, v√† li√™n h·ªá v·ªõi Topic 2 (Review & Sentiment Analysis) ƒë·ªÉ b·∫°n d√πng cho b√°o c√°o ho·∫∑c ch·ªânh s·ª≠a cho t·ªët h∆°n.

1. M·ª•c ti√™u t·ªïng th·ªÉ c·ªßa ƒëo·∫°n code
üëâ ƒêo·∫°n code n√†y d√πng ƒë·ªÉ crawl n·ªôi dung b√†i vi·∫øt / comment tr√™n Tinhte.vn b·∫±ng Selenium + BeautifulSoup, sau ƒë√≥:

L·∫•y username

Tr√≠ch xu·∫•t n·ªôi dung comment th·ª±c s·ª± (lo·∫°i b·ªè metadata, n√∫t like, gi·ªù ƒëƒÉng, icon‚Ä¶)

L√†m s·∫°ch text

L∆∞u k·∫øt qu·∫£ ra file JSON

üìå R·∫•t ph√π h·ª£p cho:

Thu th·∫≠p review / opinion

L√†m sentiment analysis

√Åp d·ª•ng tr·ª±c ti·∫øp cho CH·ª¶ ƒê·ªÄ 2: PH√ÇN T√çCH REVIEW & C·∫¢M X√öC

2. Ph√¢n t√≠ch t·ª´ng kh·ªëi logic quan tr·ªçng
üîπ 2.1. Leo ng∆∞·ª£c DOM ƒë·ªÉ t√¨m container ch·ª©a n·ªôi dung
for _ in range(5):  # Thu 5 cap parent
    if current is None:
        break
üëâ √ù t∆∞·ªüng:

Do HTML c·ªßa Tinhte kh√¥ng c·ªë ƒë·ªãnh, n√™n:

T·ª´ node hi·ªán t·∫°i ‚Üí ƒëi l√™n t·ªëi ƒëa 5 c·∫•p cha

T√¨m container c√≥ nhi·ªÅu text nh·∫•t

text = current.get_text(separator="\n", strip=True)

if len(text) > len(content_text):
    content_text = text
üìå ƒê√¢y l√† heuristic th√¥ng minh:

Container ch·ª©a comment th·∫≠t th∆∞·ªùng c√≥ text d√†i nh·∫•t

Tr√°nh ph·ª• thu·ªôc class name ‚Üí √≠t b·ªã g√£y khi web ƒë·ªïi giao di·ªán

‚úÖ ƒêi·ªÉm m·∫°nh:

Linh ho·∫°t

√çt hard-code

‚ö†Ô∏è H·∫°n ch·∫ø:

C√≥ th·ªÉ l·∫•y th·ª´a text (quote, ch·ªØ k√Ω, ch·ªØ trong n√∫t)

üîπ 2.2. T√°ch text th√†nh c√°c d√≤ng
lines = content_text.split('\n')
üëâ Chuy·ªÉn block text th√†nh danh s√°ch d√≤ng ƒë·ªÉ:

D·ªÖ l·ªçc metadata

D·ªÖ ph√°t hi·ªán n·ªôi dung th·∫≠t

3. Logic ph√°t hi·ªán n·ªôi dung comment th·∫≠t
üîπ 3.1. D√πng username l√†m m·ªëc b·∫Øt ƒë·∫ßu
if username in line:
    found_user = True
    continue
üëâ Gi·∫£ ƒë·ªãnh:

N·ªôi dung comment lu√¥n n·∫±m sau d√≤ng ch·ª©a username

‚úÖ H·ª£p l√Ω v·ªõi c·∫•u tr√∫c forum
‚ö†Ô∏è C√≥ th·ªÉ sai n·∫øu username xu·∫•t hi·ªán trong n·ªôi dung quote

üîπ 3.2. B·ªô l·ªçc metadata (r·∫•t quan tr·ªçng)
skip_words = ['G√Ä', 'ƒê·∫†I B√ÄNG', 'B√°o x·∫•u', 'ng√†y', 'gi·ªù', 'Th√≠ch',
              'Kh√¥ng th√≠ch', 'Share', 'FactTinhte', 'ƒë√£ n√≥i', 'said']
üëâ Lo·∫°i b·ªè:

C·∫•p ƒë·ªô user (G√†, ƒê·∫°i B√†ng‚Ä¶)

Th·ªùi gian ƒëƒÉng

N√∫t t∆∞∆°ng t√°c

Quote

üìå ƒê√¢y l√† rule-based cleaning, r·∫•t hay cho ti·∫øng Vi·ªát.

üîπ 3.3. C√°c rule l·ªçc n·ªôi dung r√°c
if re.match(r'^\d+$', line):
    continue
‚Üí Lo·∫°i d√≤ng ch·ªâ c√≥ s·ªë (like count)

if len(line) < 10:
    continue
‚Üí Tr√°nh m·∫•y c√¢u ki·ªÉu ‚Äúok‚Äù, ‚Äúup‚Äù, ‚Äú=))‚Äù

üëâ ƒêi·ªÅu n√†y gi√∫p:

N·ªôi dung crawl ch·∫•t l∆∞·ª£ng h∆°n

Ph√π h·ª£p cho NLP

üîπ 3.4. Gi·ªõi h·∫°n t·ªëi ƒëa 3 d√≤ng n·ªôi dung
if len(content_lines) >= 3:
    break
üìå M·ª•c ƒë√≠ch:

Tr√°nh l·∫•y qu√° d√†i

Gi·ªØ n·ªôi dung s√∫c t√≠ch (review-style)

‚ö†Ô∏è N·∫øu l√†m sentiment n√¢ng cao ‚Üí c√≥ th·ªÉ b·ªè gi·ªõi h·∫°n n√†y

4. L√†m s·∫°ch vƒÉn b·∫£n (Text Cleaning)
content = re.sub(r'@\w+', '', content)
‚Üí X√≥a mention

content = re.sub(r'[‚Üë‚Üí‚Üê‚Üì]', '', content)
‚Üí X√≥a icon ƒëi·ªÅu h∆∞·ªõng

content = re.sub(r'\s+', ' ', content)
‚Üí Chu·∫©n h√≥a kho·∫£ng tr·∫Øng

‚úÖ ƒê√∫ng chu·∫©n pipeline NLP

5. ƒêi·ªÅu ki·ªán l∆∞u d·ªØ li·ªáu
if content and len(content) > 15:
üëâ Ch·ªâ l∆∞u:

C√≥ n·ªôi dung

ƒê·ªß d√†i ƒë·ªÉ ph√¢n t√≠ch c·∫£m x√∫c

processed.add(username)
‚Üí Tr√°nh crawl tr√πng user

6. Output d·ªØ li·ªáu
data.append({
    "index": len(data) + 1,
    "username": username,
    "content": content
})
üìå Format n√†y r·∫•t ph√π h·ª£p ƒë·ªÉ:

L√†m sentiment analysis

Keyword extraction

Topic modeling

7. Li√™n h·ªá tr·ª±c ti·∫øp v·ªõi CH·ª¶ ƒê·ªÄ 2
B·∫°n ƒë√£ c√≥ s·∫µn:
‚úÖ content ‚Üí ph√¢n t√≠ch c·∫£m x√∫c
‚ùå Ch∆∞a c√≥ rating, date, shop_name

üëâ C√≥ th·ªÉ m·ªü r·ªông:

Map keyword ‚Üí rating (rule-based)

Crawl th√™m:

Ng√†y ƒëƒÉng

Topic s·∫£n ph·∫©m / b√†i vi·∫øt

8. ƒê√°nh gi√° t·ªïng quan (ƒë·ªÉ vi·∫øt v√†o b√°o c√°o)
‚úÖ ƒêi·ªÉm m·∫°nh
Kh√¥ng ph·ª• thu·ªôc class HTML

L·ªçc r√°c t·ªët

Ph√π h·ª£p ti·∫øng Vi·ªát

Data s·∫°ch ƒë·ªÉ NLP

‚ö†Ô∏è ƒêi·ªÉm h·∫°n ch·∫ø
Rule-based ‚Üí kh√≥ t·ªïng qu√°t

Ch∆∞a c√≥ sentiment label

Ch∆∞a x·ª≠ l√Ω emoji (üòäüò°)

9. G·ª£i √Ω n√¢ng c·∫•p (ƒÉn ƒëi·ªÉm cao)
Th√™m:

emoji.demojize(content)
Th√™m sentiment score (VADER / PhoBERT)

So s√°nh:

Comment d√†i vs ng·∫Øn

Toxic keyword vs sentiment

N·∫øu b·∫°n mu·ªën, m√¨nh c√≥ th·ªÉ:

üî• Vi·∫øt m√¥ t·∫£ thu·∫≠t to√°n cho b√°o c√°o

üî• Chuy·ªÉn ƒëo·∫°n n√†y th√†nh flowchart

üî• G·ª£i √Ω c√°ch map code n√†y v√†o Topic 2

üî• Refactor code cho g·ªçn & s·∫°ch h∆°n

üëâ B·∫°n ƒëang d√πng ƒëo·∫°n n√†y cho b√°o c√°o m√¥n n√†o hay ƒë·ªì √°n cu·ªëi k·ª≥?

B·∫°n ƒë√£ n√≥i:
crawl ady.py
Python
ƒë·ªçc file n√†y chuy·ªÉn t·ª´ web tinh te, otofun sang shopeefood foody ggmap
ChatGPT ƒë√£ n√≥i:
M√¨nh ƒë√£ ƒë·ªçc file crawl ady.py c·ªßa b·∫°n (crawl t·ª´ Otofun/Tinhte ‚Äì d·∫°ng forum) 
crawl ady

.
D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch + c√°ch CHUY·ªÇN T∆Ø DUY & CODE sang ShopeeFood / Foody / Google Maps Reviews cho Topic 2 ‚Äì Review & Sentiment.

1. So s√°nh b·∫£n ch·∫•t d·ªØ li·ªáu: Forum vs Review Platform
ƒê·∫∑c ƒëi·ªÉm	Tinhte / Otofun (Forum)	ShopeeFood / Foody / GG Maps
C·∫•u tr√∫c	HTML tƒ©nh, d·ªÖ parse	SPA, load ƒë·ªông
N·ªôi dung	B√†i vi·∫øt d√†i, tranh lu·∫≠n	Review ng·∫Øn, c√≥ rating
Rating	‚ùå Kh√¥ng c√≥	‚≠ê C√≥ (1‚Äì5)
Th·ªùi gian	C√≥	C√≥
Shop/Place	‚ùå	‚úÖ C√≥
Crawl	requests + BS	Selenium / Playwright
üëâ Kh√¥ng th·ªÉ d√πng nguy√™n requests + BeautifulSoup cho ShopeeFood / Foody / GG Map.

2. Ph√¢n t√≠ch file hi·ªán t·∫°i c·ªßa b·∫°n (t∆∞ duy c·ªët l√µi)
File c·ªßa b·∫°n ƒëang l√†m 3 vi·ªác r·∫•t t·ªët:

username = user_tag.get_text(strip=True)
content = content_block.get_text("\n", strip=True)
üëâ T√ÅCH 3 TH√ÄNH PH·∫¶N C·ªêT L√ïI

Ai review (username)

N·ªôi dung g√¨ (content)

Thu·ªôc v·ªÅ ƒë√¢u (thread ‚Üí shop)

üëâ Khi chuy·ªÉn n·ªÅn t·∫£ng, ch·ªâ thay ph·∫ßn SELECTOR & C√ÅCH LOAD PAGE, logic x·ª≠ l√Ω gi·ªØ nguy√™n.

3. Chuy·ªÉn sang ShopeeFood
3.1. ƒê·∫∑c ƒëi·ªÉm ShopeeFood
Load b·∫±ng JS

Review n·∫±m trong modal / tab

C√≥:

username

rating (‚≠ê)

date

content

3.2. C√¥ng ngh·ªá b·∫Øt bu·ªôc
‚úÖ Selenium / Playwright
‚ùå requests (kh√¥ng load ƒë∆∞·ª£c)

3.3. Mapping code (t∆∞ duy)
Otofun	ShopeeFood
THREAD_URL	SHOP_URL
article.message	div.review-item
.message-name	span.username
.bbWrapper	div.review-content
Kh√¥ng c√≥ rating	‚≠ê span.rating
3.4. Pseudo-code ShopeeFood (chu·∫©n b√°o c√°o)
driver.get(SHOP_URL)
sleep(5)

reviews = driver.find_elements(By.CSS_SELECTOR, "div.review-item")

for r in reviews:
    username = r.find_element(By.CSS_SELECTOR, ".username").text
    rating = len(r.find_elements(By.CSS_SELECTOR, ".icon-star.active"))
    content = r.find_element(By.CSS_SELECTOR, ".review-content").text
    date = r.find_element(By.CSS_SELECTOR, ".review-date").text
üëâ Output JSON:

{
  "shop_name": "...",
  "platform": "ShopeeFood",
  "username": "...",
  "rating": 1-5,
  "content": "...",
  "date": "..."
}
4. Chuy·ªÉn sang Foody
4.1. ƒê·∫∑c ƒëi·ªÉm Foody
HTML b√°n ƒë·ªông

C√≥ th·ªÉ:

requests (trang ƒë·∫ßu)

Selenium (load th√™m)

4.2. Mapping selector
Otofun	Foody
article.message	div.review-item
username	span.fd-user
content	div.rd-des
rating	div.rating-point
4.3. Pseudo-code Foody
reviews = soup.select("div.review-item")

for r in reviews:
    username = r.select_one(".fd-user").get_text(strip=True)
    rating = float(r.select_one(".rating-point").text)
    content = r.select_one(".rd-des").get_text(" ", strip=True)
üìå Foody r·∫•t h·ª£p cho:

Crawl nhanh

Dataset l·ªõn

Ph√¢n t√≠ch keyword

5. Chuy·ªÉn sang Google Maps Reviews (ƒÉn ƒëi·ªÉm cao)
5.1. ƒê·∫∑c ƒëi·ªÉm
100% JS

DOM thay ƒë·ªïi li√™n t·ª•c

Scroll m·ªõi load review

5.2. B·∫Øt bu·ªôc
‚úÖ Selenium
‚ùå requests

5.3. Logic crawl GG Map
# Scroll review box
for _ in range(10):
    driver.execute_script(
        "document.querySelector('.m6QErb').scrollTop = 100000"
    )
    sleep(1)
5.4. Extract review
reviews = driver.find_elements(By.CSS_SELECTOR, "div.jftiEf")

for r in reviews:
    username = r.find_element(By.CLASS_NAME, "d4r55").text
    rating = r.find_element(By.CLASS_NAME, "kvMYJc").get_attribute("aria-label")
    content = r.find_element(By.CLASS_NAME, "MyEned").text
    date = r.find_element(By.CLASS_NAME, "rsqaWe").text
6. So s√°nh OUTPUT ‚Äì ph·ª•c v·ª• Topic 2
Platform	C√≥ rating	C√≥ date	Ph√π h·ª£p sentiment
Otofun	‚ùå	‚ùå	‚ö†Ô∏è
Tinhte	‚ùå	‚ö†Ô∏è	‚ö†Ô∏è
ShopeeFood	‚úÖ	‚úÖ	‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
Foody	‚úÖ	‚úÖ	‚≠ê‚≠ê‚≠ê‚≠ê
Google Maps	‚úÖ	‚úÖ	‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
7. K·∫øt lu·∫≠n (vi·∫øt v√†o b√°o c√°o)
Code crawl ban ƒë·∫ßu ƒë∆∞·ª£c x√¢y d·ª±ng cho forum (Otofun/Tinhte).
Khi chuy·ªÉn sang n·ªÅn t·∫£ng review (ShopeeFood, Foody, Google Maps), nh√≥m gi·ªØ nguy√™n pipeline x·ª≠ l√Ω n·ªôi dung, nh∆∞ng thay ƒë·ªïi c√°ch load d·ªØ li·ªáu v√† selector HTML, ƒë·ªìng th·ªùi b·ªï sung c√°c tr∆∞·ªùng quan tr·ªçng nh∆∞ rating, th·ªùi gian v√† t√™n c·ª≠a h√†ng, gi√∫p d·ªØ li·ªáu ph√π h·ª£p h∆°n cho ph√¢n t√≠ch c·∫£m x√∫c v√† xu h∆∞·ªõng ƒë√°nh gi√° kh√°ch h√†ng.

8. M√¨nh c√≥ th·ªÉ l√†m ti·∫øp cho b·∫°n
‚úÖ Vi·∫øt 1 file crawl chung cho 3 platform

‚úÖ Chu·∫©n h√≥a schema JSON

‚úÖ Vi·∫øt ph·∫ßn m√¥ t·∫£ chuy·ªÉn ƒë·ªïi h·ªá th·ªëng cho b√°o c√°o

‚úÖ Refactor code forum ‚Üí review platform

üëâ B·∫°n ƒëang l√†m m·ªôt m√¨nh hay theo nh√≥m? M√¨nh s·∫Ω vi·∫øt ƒë√∫ng level ƒë·ªÉ b·∫°n ƒÉn ƒëi·ªÉm t·ªëi ƒëa.


Ngu·ªìn
B·∫°n ƒë√£ n√≥i:
vi·∫øt 1 file crawl chung c ho
ChatGPT ƒë√£ n√≥i:
Ok, m√¨nh s·∫Ω vi·∫øt 1 FILE CRAWL CHUNG (UNIFIED CRAWLER) cho **Topic 2 ‚Äì Review

B·∫°n ƒë√£ n√≥i:
vi·∫øt 1 file crawl m·ªõi cho ƒë√∫ng v·ªõi y√™u c·∫ßu c·ªßa topic 2 trong ·∫£nh ƒë√£ g·ª≠i ·ªü ƒë·∫ßu trang 
ChatGPT ƒë√£ n√≥i:
OK üëç D∆∞·ªõi ƒë√¢y l√† 1 FILE CRAWL M·ªöI ‚Äì VI·∫æT L·∫†I T·ª™ ƒê·∫¶U, ƒë√∫ng 100% y√™u c·∫ßu CH·ª¶ ƒê·ªÄ 2: PH√ÇN T√çCH REVIEW & C·∫¢M X√öC trong ·∫£nh b·∫°n g·ª≠i.

M·ª•c ti√™u file n√†y:

Crawl review kh√°ch h√†ng

√Åp d·ª•ng cho ShopeeFood / Foody / Google Maps

Thu ƒë∆∞·ª£c ƒë√∫ng schema d·ªØ li·ªáu ƒë·ªÉ l√†m ph√¢n t√≠ch (EDA, sentiment, keyword, theo th·ªùi gian)

üéØ Y√äU C·∫¶U TOPIC 2 (ƒë√£ map v√†o code)
D·ªØ li·ªáu thu th·∫≠p:
platform

shop_name

username

rating

content

date

üëâ ƒê·ªß ƒë·ªÉ tr·∫£ l·ªùi t·∫•t c·∫£ c√¢u h·ªèi c·ª©ng & m·ªü c·ªßa Topic 2

üìÅ FILE: crawl_reviews_topic2.py
import time
import json
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options


########################################
# CONFIG
########################################

PLATFORM = "shopeefood"   # shopeefood | foody | ggmap
SHOP_URL = "PASTE_SHOP_URL_HERE"
LIMIT_REVIEWS = 50

OUTPUT_FILE = "reviews_topic2.json"


########################################
# UTILS
########################################

def clean_text(text: str) -> str:
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'@\w+', '', text)
    return text.strip()


########################################
# DRIVER
########################################

def init_driver():
    options = Options()
    options.add_argument("--start-maximized")
    options.add_argument("--disable-notifications")
    return webdriver.Chrome(options=options)


########################################
# SHOPEEFOOD
########################################

def crawl_shopeefood(driver, url, limit):
    print("Crawling ShopeeFood...")
    driver.get(url)
    time.sleep(6)

    reviews_data = []

    # Scroll ƒë·ªÉ load review
    for _ in range(10):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")
        time.sleep(1)

    reviews = driver.find_elements(By.CSS_SELECTOR, "div.review-item")

    for r in reviews[:limit]:
        try:
            username = r.find_element(By.CSS_SELECTOR, ".username").text
            content = r.find_element(By.CSS_SELECTOR, ".review-content").text
            stars = r.find_elements(By.CSS_SELECTOR, ".icon-star.active")
            rating = len(stars)
            date = r.find_element(By.CSS_SELECTOR, ".review-date").text

            reviews_data.append({
                "platform": "ShopeeFood",
                "shop_name": "",
                "username": username,
                "rating": rating,
                "content": clean_text(content),
                "date": date
            })

        except Exception:
            continue

    return reviews_data


########################################
# FOODY
########################################

def crawl_foody(driver, url, limit):
    print("Crawling Foody...")
    driver.get(url)
    time.sleep(5)

    reviews_data = []

    reviews = driver.find_elements(By.CSS_SELECTOR, "div.review-item")

    for r in reviews[:limit]:
        try:
            username = r.find_element(By.CSS_SELECTOR, ".fd-user").text
            content = r.find_element(By.CSS_SELECTOR, ".rd-des").text
            rating = float(r.find_element(By.CSS_SELECTOR, ".rating-point").text)
            date = r.find_element(By.CSS_SELECTOR, ".fd-time").text

            reviews_data.append({
                "platform": "Foody",
                "shop_name": "",
                "username": username,
                "rating": rating,
                "content": clean_text(content),
                "date": date
            })

        except Exception:
            continue

    return reviews_data


########################################
# GOOGLE MAPS
########################################

def crawl_google_maps(driver, url, limit):
    print("Crawling Google Maps...")
    driver.get(url)
    time.sleep(8)

    reviews_data = []

    # Scroll review box
    for _ in range(15):
        driver.execute_script(
            "document.querySelector('.m6QErb').scrollTop = 100000"
        )
        time.sleep(1)

    reviews = driver.find_elements(By.CSS_SELECTOR, "div.jftiEf")

    for r in reviews[:limit]:
        try:
            username = r.find_element(By.CLASS_NAME, "d4r55").text
            rating_text = r.find_element(By.CLASS_NAME, "kvMYJc").get_attribute("aria-label")
            rating = int(re.search(r'\d', rating_text).group())
            content = r.find_element(By.CLASS_NAME, "MyEned").text
            date = r.find_element(By.CLASS_NAME, "rsqaWe").text

            reviews_data.append({
                "platform": "GoogleMaps",
                "shop_name": "",
                "username": username,
                "rating": rating,
                "content": clean_text(content),
                "date": date
            })

        except Exception:
            continue

    return reviews_data


########################################
# MAIN
########################################

def main():
    driver = init_driver()

    try:
        if PLATFORM == "shopeefood":
            data = crawl_shopeefood(driver, SHOP_URL, LIMIT_REVIEWS)

        elif PLATFORM == "foody":
            data = crawl_foody(driver, SHOP_URL, LIMIT_REVIEWS)

        elif PLATFORM == "ggmap":
            data = crawl_google_maps(driver, SHOP_URL, LIMIT_REVIEWS)

        else:
            raise ValueError("Platform kh√¥ng h·ª£p l·ªá")

        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"ƒê√£ l∆∞u {len(data)} review v√†o {OUTPUT_FILE}")

    finally:
        driver.quit()


if __name__ == "__main__":
    main()
üìä FILE N√ÄY ƒê√ÅP ·ª®NG G√å CHO TOPIC 2?
‚úÖ C√¢u h·ªèi C·ª®NG
Rating 1‚≠ê vs 5‚≠ê theo th·ªùi gian ‚Üí d√πng rating + date

Keyword 1‚≠ê vs 5‚≠ê ‚Üí d√πng content

‚úÖ C√¢u h·ªèi M·ªû
ƒê·ªô d√†i comment vs s·ªë sao ‚Üí len(content)

So s√°nh v√πng mi·ªÅn ‚Üí shop_name / location (b·ªï sung)

üß† G·ª¢I √ù VI·∫æT B√ÅO C√ÅO (ƒÉn ƒëi·ªÉm)
Nh√≥m x√¢y d·ª±ng m·ªôt crawler th·ªëng nh·∫•t cho c√°c n·ªÅn t·∫£ng ƒë√°nh gi√° ph·ªï bi·∫øn (ShopeeFood, Foody, Google Maps). D·ªØ li·ªáu thu th·∫≠p bao g·ªìm n·ªôi dung ƒë√°nh gi√°, s·ªë sao, th·ªùi gian v√† ng∆∞·ªùi d√πng, ph·ª•c v·ª• cho ph√¢n t√≠ch c·∫£m x√∫c, xu h∆∞·ªõng ƒë√°nh gi√° theo th·ªùi gian v√† tr√≠ch xu·∫•t t·ª´ kh√≥a ƒë·∫∑c tr∆∞ng.

üëâ B·∫°n mu·ªën m√¨nh l√†m ti·∫øp ph·∫ßn n√†o?
üî• Chu·∫©n h√≥a EDA + bi·ªÉu ƒë·ªì

üî• Sentiment analysis ti·∫øng Vi·ªát

üî• File report.ipynb

üî• Ch·ªânh code cho ch·∫°y ch·∫Øc ch·∫Øn theo t·ª´ng platform

Ch·ªâ c·∫ßn n√≥i: ‚Äúl√†m ti·∫øp ph·∫ßn ‚Ä¶
